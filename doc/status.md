# Отчет о состоянии проекта

---

## Подготовка данных:

- **Загрузка данных:** Исходные данные успешно загружены из директории с HTML файлами с использованием модуля `DirectoryLoader`.
- **Очистка текста:** Текст каждого документа был очищен от стоп-слов, пунктуации и ненужных метаданных с помощью библиотеки `spaCy`. Для этого был создан цикл, который проходил по каждому документу и удалял указанные элементы. Важно! Для корректной работы, после установки зависимостей, необходимо выполнить `python -m spacy download ru_core_news_sm`
- **Сохранение данных:** Очищенные документы были сохранены в формате `pickle` с использованием метода `dump`.

## Создание и использование модели:

- **Загрузка данных:** Очищенные документы были успешно загружены из файла `pickle` с использованием метода `load`.
- **Разбиение на чанки:** Каждый документ был разделен на чанки с помощью `RecursiveCharacterTextSplitter` для обеспечения удобства обработки больших объемов текста.
- **Создание эмбеддингов:** Для каждого чанка были созданы эмбеддинги с помощью модели `HuggingFaceEmbeddings`. Это позволило преобразовать текстовые данные в векторные представления, подходящие для дальнейшего анализа.
- **Создание базы данных:** На основе полученных эмбеддингов была создана база данных FAISS с использованием метода `from_documents`. Это обеспечило быстрый и эффективный поиск похожих документов.
- **Поиск релевантных документов:** Был проведен поиск релевантных документов по заданной теме с использованием метода `similarity_search`.
- **Вывод результатов:** Полученные результаты были отформатированы в виде текста с помощью регулярных выражений и выведены на экран.

## Генерация ответов на основе вопросов:

- В коде используется модель LLM для генерации ответов на основе заданных вопросов. Модель обучена на основе шаблона запроса, правил генерации ответов и контекста вопроса. При генерации ответов учитываются данные из базы знаний и контекст вопроса, чтобы обеспечить более точные и информативные ответы.

- Ответы, сгенерированные моделью, формулируются на русском языке в соответствии с требованиями проекта. Это позволяет пользователям легко понимать и использовать информацию, полученную из системы.

- Параметр температуры используется для управления степенью случайности в сгенерированных ответах. Это позволяет балансировать между разнообразием ответов и их качеством в зависимости от потребностей проекта.

- Устанавливается максимальная длина текста ответа, чтобы обеспечить его читаемость и точность. Это важно для того, чтобы ответы были информативными и легко воспринимаемыми пользователями.

- Параметр top_p используется для управления вероятностным выбором токенов при генерации ответов. Это позволяет модели выбирать наиболее вероятные токены с учетом контекста вопроса и правил генерации ответов.

## Создание объекта PromptTemplate и инициализация модели LlamaCpp:

- Определен шаблон запроса для использования в LLMChain. Шаблон содержит правила для генерации ответов и переменные для вопроса и контекста.

- Модель LlamaCpp была инициализирована с заданными параметрами, включая путь к модели, температуру, максимальное количество токенов и длину текста.

## Создание объекта LLMChain:

- Создан объект LLMChain с использованием модели и шаблона запроса для генерации ответов на основе вопросов.

Этапы процесса выполнены успешно, обеспечивая подготовку данных, создание модели и генерацию ответов для эффективного поиска информации из корпоративной документации.

---